决策树作为最基础最常见的有监督学习模型、常被用于分类问题和回归问题，在市场营销和生物医药领域尤其受欢迎

决策树三要素：特征选择、生成树、剪枝

特征选择：从训练数据众多的特征中选择一个特征作为当前节点的分裂标准

决策树生成：根据选择的特征评估标准，从上到下递归地生成子节点，直至数据集不可分则停止生长

剪枝：决策树容易过拟合，剪枝来缩小树结构规模、缓解过拟合



决策树优缺点？

优点：容易理解、时间复杂度小、 多输出、一次构建多次使用、用于小数据集、对缺失值不敏感

缺点：容易过拟合、对连续性字段难预测、类别多时错误率增加快、处理关联性强的数据表现不好



信息熵,用来度量样本集合纯度的常用指标，信息熵越小，D纯度越高
$$
Ent(D)=-\sum_{k=1}^{|y|}p_klog_2p_k
$$
信息增益，计算属性a对样本集D进行划分所获得的信息增益,ID3算法使用其作为准则，但对可取值数目较多的属性有所偏好
$$
Gain(D,a)= Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)
$$
增益率：对可取值数目较少的属性有所偏好,C4.5算法采用它
$$
Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}
$$

$$
IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
$$

基尼系数：CART决策树使用这，基尼系数越小，数据集纯度越高
$$
Gini(D)=1-\sum_{k=1}^{|y|}p_k^2
$$
对决策树进行剪枝：预剪枝和后剪枝，这两种方法如何进行？有什么有优缺点？

预剪枝的核心思想是在树中结点进行扩展之前，先计算当前的划分是否能带来模型泛化能力的提升，如果不能，则不再继续生长子树

优点：思想直接、算法简单、效率高，适合解决大规模问题

缺点：有欠拟合的风险、对于准确地估计何时停止树的生长，针对不同问题会有很大差别，需要一定的经验判断

后剪枝的核心思想是让算法生成一颗完全生长的决策树，然后从最底层向上计算是否剪枝

优点：可以得到泛化能力更强的决策树

缺点：时间开销会更大



常见的后剪枝方法：错误率降低剪枝、悲观剪枝、代价复杂度剪枝、最小误差剪枝