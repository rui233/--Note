### SVM (Support Vector Machine)支持向量机

SVM是一种二分类模型，它是定义在特征空间上的、间隔最大的线性分类器

SVM可以处理线性可分数据集和线性不可分数据集

由于支撑向量在确定划分超平面中起着决定性作用，所以将这些模型称为支持向量机



1. 给定一个特征空间上的训练数据集{($\vec{x_n},\widetilde{y_n}$)} ，其中y$\in \{ +1,-1\}$ 。

    $\vec{x}_i $为第  $i$个特征向量，也称作实例；  $\widetilde{y}_i $为$ \vec{x}_i $  的类标记； ($\vec{x}_i,\widetilde{y}_i $) 称作样本点。

   - 当$\widetilde{y}_i  =+1$   时，称 $\vec{x}_i$ 为正例。
   - 当 $\widetilde{y}_i =-1$  时，称  $\vec{x}_i$为负例。

   假设训练数据集是线性可分的，则学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。

   分离超平面对应于方程$\vec{w}^T\vec{x}+b=0$ ， 它由法向量$\vec{w}$  和截距b  决定，可以用  $(\vec{w}, b )$来表示。

2. 给定线性可分训练数据集，通过间隔最大化学习得到的分离超平面为： ，

   相应的分类决策函数： ，称之为线性可分支持向量机。

3. 当训练数据集线性可分时，存在无穷个分离超平面可以将两类数据正确分开。

   - 感知机利用误分类最小的策略，求出分离超平面。但是此时的解有无穷多个。
   - 线性可分支持向量机利用间隔最大化求得最优分离超平面，这样的解只有唯一的一个。

##### 函数间隔

1、将一个点距离划分超平面的远近来表示分类预测的可靠程度，越远越可靠，越近则没那么可靠

2、在超平面$\vec{w}^T\vec{x}+b=0$确定的情况下：

$\circ$ $|\vec{w}^T\vec{x}+b|$能够相对得表示点$\vec{x}$距离超平面的远近

$\circ$  $\vec{w}^T\vec{x}+b$的符号和类标记$\widetilde{y}_i$的符号是否一致能够表示分类是否正确





二维平面上的线性可分，两类点集被一条直线完全分开

$D_0$和$D_1$是n维欧氏空间的两个点集，如果存在n维向量w和实数b，使得所有属于$D_0$的点$x_i$都有$wx_i+b >0$,而对所有属于$D_1$的点集都有$wx_j+b<0$,则我们称$D_0$和$D_1$线性可分

将$D_0$和$D_1$完全正确分开的$wx+b=0$,就是一个超平面



最大间隔超平面：以最大间隔把两类样本分开的超平面，是最佳超平面，叫做最大间隔超平面

支撑向量：距离超平面最近的一些点，满足约束条件等号成立，即$\widetilde{y}_i(\vec{w}^T\vec{x}+b)-1=0$

​	$\circ$对于正实例点，支持向量位于超平面 $H_1$:$\vec{w}^T\vec{x}+b=1$

​	$\circ$对于负实例点，支持向量位于超平面 $H_2$:$\vec{w}^T\vec{x}+b=-1$

超平面$H_1$、$H_2$称为间隔边界，它们与划分超平面平行



SVM最优化的问题：找到最大间隔超平面

三维空间内平面的数学表示：
$$
Ax+By+Cz+D=0
$$
拓展到n维空间的超平面：$\vec{\theta_nx}=0$

等价于 
$$
\vec{w}^T\vec{x}+b=0
$$


即超平面的数学描述，超平面可被$\vec{w}$和位移b确定，记为超平面$(\vec{w},b)$



三维空间点$(x,y)$到直线$Ax+By+Cz+D = 0$的计算公式是：
$$
\frac{|Ax+By+Cz+D|}{\sqrt{A^2+B^2+C^2}}
$$
拓展到n维空间，距离d为：


$$
\frac{|\vec{\theta}^Tx|}{||\vec{\theta}||_2}
$$

$$
\frac{|\vec{w}^T\vec{x}+b|}{||\vec{w}||_2}
$$
其中 $||w||=\sqrt{\sum_{i=1}^{d}w_d^2}$

正负代表平面的方向，向上还是向下
$$
wx+b\geq \gamma,\quad y_i=+1
$$

$$
wx+b\leq-\gamma,\quad y_i=-1
$$

上式表示的是平面，除一个系数或者乘一个系数不影响该平面的性质,左右同时除以$\gamma$

变为：
$$
wx+b\geq 1,\quad y_i=+1
$$

$$
wx+b\leq -1,\quad y_i=-1
$$

可表示为
$$
y_i(\vec{w}^T +b)\geq1,i=1,2,……,m
$$
两个异类支持向量到超平面的距离和margin（间隔）为
$$
\gamma=\frac{wx+b}{||w||_2}-\frac{wx+b}{||w||_2}=\frac{1-(-1)}{||w||_2}=\frac{2}{||w||_2}
$$
​					最大间隔即$max\frac{2}{\vec{||w||}}$,为方便计算，取其倒数，相当于求其倒数的最小值，即$min \frac{1}{2}|| \vec{w}||^2$

综上，SVM的基本型：
$$
\begin{cases} min_{\vec{w},b} \frac{1}{2}|| \vec{w}||^2\\ \\s.t.\quad y_i(\vec{w}^T +b)\geq1,i=1,2,……,m \end{cases}
$$
表示找到满足约束条件的$\vec{w}$和$b$这两个参数，使得$||\vec{w}||^2$最小，$s.t.$是satisfy,表示满足该条件约束

使用拉格朗日乘子法可以将有约束的优化问题转化为无约束的优化问题
$$
L(x,y) = f(x,y)+\lambda g(x,y)
$$
即
$$
L(\vec{w},b,\lambda) = \frac{1}{2} ||\vec{w}||^2 + \sum_{i=1}^{m}\lambda_i[1-y_i(\vec{w}\vec{x}_i+b)]
$$
其中拉格朗日乘子$\lambda_i\geq0$



#### Soft Margin   软间隔

对于线性不可分训练数据，线性支持向量机不再适用，但可以想办法将它扩展到线性不可分问题。

现实任务中，不一定能找到合适的核函数使得训练样本在特征空间中线性可分

缓解该问题的办法是允许在一些样本不满足约束

即
$$
s.t.\quad y_i(\vec{w}^T +b)\geq1-\xi_i
$$


1. 设训练集为  ，其中 。

   假设训练数据集不是线性可分的，这意味着某些样本点  不满足函数间隔大于等于 1 的约束条件。

   - 对每个样本点  引进一个松弛变量$\xi_i\geq0$ ，使得函数间隔加上松弛变量大于等于 1。

     即约束条件变成了：$y_i(\vec{w}^T +b)\geq1-\xi_i$  。

   - 对每个松弛变量 $\xi_i$ ，支付一个代价$\xi_i$ 。目标函数由原来的 $\frac{1}{2}|| \vec{w}||^2$ 变成：
     $$
     min_{\vec{w},b} \frac{1}{2}|| \vec{w}||^2+C\sum_{i=1}^{N}\xi_i
     $$
     这里 $C>0$ 称作惩罚参数，一般由应用问题决定。

     -  $C$值大时，对误分类的惩罚增大，此时误分类点凸显的更重要
     - $C$ 值较大时，对误分类的惩罚增加，此时误分类点比较重要。
     - $C$ 值较小时，对误分类的惩罚减小，此时误分类点相对不重要。

2. 相对于硬间隔最大化，上式则称为软间隔最大化。

   于是线性不可分的线性支持向量机的学习问题变成了凸二次规划问题：
   $$
   min_{\vec{w},b} \frac{1}{2}|| \vec{w}||^2+C\sum_{i=1}^{N}\xi_i
   $$

   $$
   s.t.\quad y_i(\vec{w}^T +b)\geq1-\xi_i，\quad i=1,2,...,N
   $$

   $$
   \xi_i\geq0,\quad i=1,2,...,N
   $$

   

   - 这称为线性支持向量机的原始问题。

   - 因为这是个凸二次规划问题，因此解存在。

     可以证明  $\vec{w} $的解是唯一的；  $b$的解不是唯一的， $b$的解存在于一个区间。

- 线性支持向量机包含线性可分支持向量机。
- 现实应用中训练数据集往往是线性不可分的，线性支持向量机具有更广泛的适用性。





核函数可以解决原始特征空间的线性不可分问题，有非线性效果，产生复杂边界



Question:

1、在空间上线性可分的两类点，分别向SVM分类的超平面上做投影，这些点在超平面的投影仍然是线性可分的吗？

不是，对于任意线性可分的两组点，它们在SVM分类的超平面的投影都是线性不可分的

举反例：在二维空间时，投影会是超平面的同一个点

2、是否存在一组参数使SVM训练误差为0？

存在

3、训练误差为0的SVM分类器一定存在吗？

存在

4、加入松弛变量的SCM训练误差可以为0吗？

C取0时，w取0可达到优化目标，但此时误差不一定为0

